{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\ML\\RAG\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import PyPDF2\n",
    "import torch\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdf_file = \"./documents/epix_(Gen_2)_Series_OM_EN-US.pdf\"\n",
    "pdf_file = \"./documents/Teric_Manual-metric_2021.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from PDF\n",
    "def extract_text(pdf_path):\n",
    "\n",
    "\textracted_text = []\n",
    "\n",
    "\twith open(pdf_path, \"rb\") as pdf_file:\n",
    "\n",
    "\t\t# Instantiate PyPDF reader\n",
    "\t\treader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "\t\t# Parse each page\n",
    "\t\tfor page in reader.pages:\n",
    "\n",
    "\t\t\t# Extract page text\n",
    "\t\t\tpage_text = page.extract_text().encode(\"utf-8\", \"xmlcharrefreplace\")\n",
    "\t\t\tpage_text = page_text.decode(\"utf-8\")\n",
    "\n",
    "\t\t\t# Replace known characters\n",
    "\t\t\tpage_text = page_text.replace(\"\\n\", \" \")\n",
    "\t\t\tpage_text = page_text.replace(\"\\xa0\", \"\")\n",
    "\t\t\tpage_text = page_text.replace(\"\\xad \", \"\")\n",
    "\n",
    "\t\t\t# Split the page text into sentences\n",
    "\t\t\tsentences = sent_tokenize(page_text)\n",
    "\t\t\textracted_text.extend(sentences)\n",
    "\n",
    "\treturn extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 325\n"
     ]
    }
   ],
   "source": [
    "# Extract text from PDF file\n",
    "extracted_text = extract_text(pdf_file)\n",
    "print(f\"Number of sentences: {len(extracted_text)}\")\n",
    "#print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(sentences):\n",
    "\n",
    "\t# Initialize encoding model\n",
    "\tmodel = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "\t# Generate sentence embeddings\n",
    "\tsentence_embeddings = model.encode(sentences)\n",
    "\n",
    "\treturn sentences, sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\ML\\RAG\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(325, 768)\n"
     ]
    }
   ],
   "source": [
    "# Vectorize text\n",
    "sentences, sentence_embeddings = vectorize_text(extracted_text)\n",
    "\n",
    "print(sentence_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate RAG response\n",
    "def generate_response(query_text, context):\n",
    "\n",
    "\t# Pass query and context to LLM\n",
    "\tsystem_message = f\"\"\"\n",
    "\t\tYou are a knowledgeable assistant tasked with answering questions based solely on the provided context.\n",
    "\t\tYour responses should be strictly based on the information contained in the context.\n",
    "\t\tIf the answer is not clear from the context, you can extrapolate from the information available in the context.\n",
    "\t\tIf the answer is not in the context, respond with \"The answer is not available in the provided context.\"\n",
    "\t\tHere is the context you should use:\n",
    "\t\t{context}\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t# Generate response\n",
    "\tresponse = ollama.chat(model=\"llama3.1\", messages=[\n",
    "\t\t{\n",
    "\t\t\t\"role\": \"system\",\n",
    "\t\t\t\"content\": system_message\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"role\": \"user\",\n",
    "\t\t\t\"content\": f\"{query_text}\"\n",
    "\t\t}\n",
    "\t])\n",
    "\n",
    "\treturn response[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the knowledge base\n",
    "def query_kb(query_text, sentence_embeddings, show_context=False):\n",
    "\n",
    "\t# Generate embeddings for the query text\n",
    "\tmodel = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\tquery_embeddings = model.encode(query_text)\n",
    "\n",
    "\t# Generate tensor of similarities between query and sentences\n",
    "\tsimilarities = model.similarity(query_embeddings, sentence_embeddings)\n",
    "\t\n",
    "\t# Extract top k similar sentences\n",
    "\tk = 25\n",
    "\ttop_k_indices = torch.topk(similarities, k).indices.tolist()\n",
    "\ttop_k_indices = [item for sublist in top_k_indices for item in sublist]\n",
    "\trelevant_sentences = [sentences[i] for i in top_k_indices]\n",
    "\n",
    "\t# Generate LLM response using similar sentences as context\n",
    "\tresponse = generate_response(query_text, relevant_sentences)\n",
    "\n",
    "\t# DEBUG\n",
    "\tif show_context:\n",
    "\t\tprint(\"#### CONTEXT ####\")\n",
    "\t\tprint(\"\\n\".join(relevant_sentences))\n",
    "\t\tprint(\"####################\\n\")\n",
    "\n",
    "\treturn response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epix Questions\n",
    "\n",
    "#query_text = \"How deep under water can I swim wearing the watch?\"\n",
    "#query_text = \"How much water pressure can the watch withstand?\"\n",
    "#query_text = \"How do I navigate back home when I'm on a run?\"\n",
    "#query_text = \"How do I get a SpO2 measurement?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teric Questions\n",
    "query_text = \"How often should I charge the device?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\ML\\RAG\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The instructions do not provide information on how often to charge the device other than stating that you can charge it with any USB wall power adapter or with a computer and that charging time is approximately 1-5 hours with Quick Charge enabled, and 3-4 hours without. However, in the context of battery care for long term storage, it is suggested to \"Top up the Teric battery every 6 months\".\n"
     ]
    }
   ],
   "source": [
    "response = query_kb(query_text, sentence_embeddings, show_context=False)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
